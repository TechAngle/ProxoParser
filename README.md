# ProxoParser - Parsing Free Proxies from Websites ðŸŒ

ProxoParser is a Python script dedicated to the task of scraping and parsing free proxy information from various websites. It relies on the use of two primary libraries, requests and bs4 (BeautifulSoup), to accomplish its web scraping functionality efficiently.

## Requirements âš ï¸

Make sure you have the following Python libraries installed before using ProxoParser:

- Python >= 3.10
- requests
- bs4 (BeautifulSoup)
- colorama
- fake_useragent
- pyfiglet

You can install these libraries using pip if they are not already installed:

```shell
pip install requests bs4 colorama fake_useragent pyfiglet
```
# Usage
1. Clone the ProxoParser repository to your local machine.

2. Configure the list of websites to scrape by editing the config.py file. This file should contain the URLs of the websites you want to scrape.

3. Run the script using the following command:

```shell
python main.py
```
4. Wait while the script parses and collects proxy information from the specified websites.

5. Once the process is complete, the proxies will be saved to a file in the parsed directory.

# Contributing
If you want to contribute to ProxoParser, feel free to fork the repository and submit a pull request with your enhancements. I welcome any improvements or new features that can make the tool more robust and efficient.

# Author
### Vladislav (TechAngle)
##### GitHub: [TechAngle](https://github.com/TechAngle/)
# Contact
If you have any questions or need assistance with ProxoParser, you can contact the author via email:

Email: rect4ngle@programmer.net

# License ðŸ–¹
This project is licensed under the **_Mozilla Public License Version 2.0_** License. See the LICENSE file for details.
